nohup: ignoring input
2025-01-10 12:05:18,823 [trainer.py] => config: ldm/ldm_dddr.yaml
2025-01-10 12:05:18,823 [trainer.py] => model_name: asp
2025-01-10 12:05:18,823 [trainer.py] => backbone_type: pretrained_vit_b16_224_vpt
2025-01-10 12:05:18,823 [trainer.py] => device: ['cuda']
2025-01-10 12:05:18,823 [trainer.py] => seed: 2024
2025-01-10 12:05:18,823 [trainer.py] => dataset: cifar224
2025-01-10 12:05:18,823 [trainer.py] => init_cls: 5
2025-01-10 12:05:18,824 [trainer.py] => increment: 5
2025-01-10 12:05:18,825 [trainer.py] => kshot: 5
2025-01-10 12:05:18,825 [trainer.py] => generator_model: CIFAR_GEN
2025-01-10 12:05:18,825 [trainer.py] => z_dim: 1000
2025-01-10 12:05:18,825 [trainer.py] => conv_dim: 64
2025-01-10 12:05:18,825 [trainer.py] => img_size: 224
2025-01-10 12:05:18,825 [trainer.py] => generator_lr: 0.005
2025-01-10 12:05:18,825 [trainer.py] => pi: 100
2025-01-10 12:05:18,825 [trainer.py] => w_bn: 50.0
2025-01-10 12:05:18,825 [trainer.py] => w_noise: 0.001
2025-01-10 12:05:18,825 [trainer.py] => server_ss: 32
2025-01-10 12:05:18,825 [trainer.py] => bn_loss: 1
2025-01-10 12:05:18,825 [trainer.py] => noise: 1
2025-01-10 12:05:18,825 [trainer.py] => ie_loss: 1
2025-01-10 12:05:18,825 [trainer.py] => act_loss: 0
2025-01-10 12:05:18,826 [trainer.py] => w_ie: 1.0
2025-01-10 12:05:18,826 [trainer.py] => w_act: 0.1
2025-01-10 12:05:18,826 [trainer.py] => syn_size: 32
2025-01-10 12:05:18,826 [trainer.py] => need_syn_imgs: True
2025-01-10 12:05:18,826 [trainer.py] => g_local_bs: 5
2025-01-10 12:05:18,826 [trainer.py] => ldm_ckpt: models/ldm/text2img-large/model.ckpt
2025-01-10 12:05:18,826 [trainer.py] => g_local_train_steps: 5
2025-01-10 12:05:18,826 [trainer.py] => dataset_syn: cifar100
2025-01-10 12:05:18,826 [trainer.py] => com_round_gen: 10
2025-01-10 12:05:18,826 [trainer.py] => g_sigma: 0
2025-01-10 12:05:18,826 [trainer.py] => save_cls_embeds: False
2025-01-10 12:05:18,826 [trainer.py] => syn_image_path: outputs/syn_image_5
2025-01-10 12:05:18,826 [trainer.py] => save_dir: outputs
2025-01-10 12:05:18,826 [trainer.py] => n_iter: 5
2025-01-10 12:05:18,826 [trainer.py] => num_users: 1
2025-01-10 12:05:18,826 [trainer.py] => cur_size: 50
2025-01-10 12:05:18,826 [trainer.py] => local_bs: 5
2025-01-10 12:05:18,826 [trainer.py] => pre_size: 48
2025-01-10 12:05:18,826 [trainer.py] => vpt_type: Deep
2025-01-10 12:05:18,826 [trainer.py] => prompt_token_num: 6
2025-01-10 12:05:18,826 [trainer.py] => avg_alpha: 0.8
2025-01-10 12:05:18,826 [trainer.py] => perturb_var: 1
2025-01-10 12:05:18,826 [trainer.py] => EMA_beta: 0.99
2025-01-10 12:05:18,826 [trainer.py] => anchor_lambda: 0.1
2025-01-10 12:05:18,826 [trainer.py] => kl_weight: 0.001
2025-01-10 12:05:18,826 [trainer.py] => TIP_init: zero
2025-01-10 12:05:18,826 [trainer.py] => tuned_epoch: 10
2025-01-10 12:05:18,826 [trainer.py] => fs_epoch: 0
2025-01-10 12:05:18,826 [trainer.py] => init_lr: 0.01
2025-01-10 12:05:18,826 [trainer.py] => fs_lr: 0.001
2025-01-10 12:05:18,826 [trainer.py] => batch_size: 48
2025-01-10 12:05:18,827 [trainer.py] => fs_batch_size: 16
2025-01-10 12:05:18,827 [trainer.py] => weight_decay: 0.0005
2025-01-10 12:05:18,827 [trainer.py] => min_lr: 0
2025-01-10 12:05:18,827 [trainer.py] => optimizer: sgd
2025-01-10 12:05:18,827 [trainer.py] => memory_size: 0
2025-01-10 12:05:18,827 [trainer.py] => memory_per_class: 0
2025-01-10 12:05:18,827 [trainer.py] => fixed_memory: False
2025-01-10 12:05:18,827 [trainer.py] => shuffle: False
2025-01-10 12:05:18,827 [trainer.py] => top_k: 5
2025-01-10 12:05:18,827 [trainer.py] => base_model_path: saved_model/asp/cifar224/5_5/asp_10_2024_pretrained_vit_b16_224_vpt.pth
2025-01-10 12:05:18,827 [trainer.py] => prefix: asp
2025-01-10 12:05:18,827 [trainer.py] => model_prefix: asp
Files already downloaded and verified
Files already downloaded and verified
2025-01-10 12:05:20,430 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
/mnt/share/yinjunhui/anaconda3/envs/cl/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:19: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_fwd(orig_func)  # type: ignore
/mnt/share/yinjunhui/anaconda3/envs/cl/lib/python3.9/site-packages/fairscale/experimental/nn/offload.py:30: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  return torch.cuda.amp.custom_bwd(orig_func)  # type: ignore
/mnt/share/yinjunhui/anaconda3/envs/cl/lib/python3.9/site-packages/pytorch_lightning/metrics/__init__.py:43: LightningDeprecationWarning: `pytorch_lightning.metrics.*` module has been renamed to `torchmetrics.*` and split off to its own package (https://github.com/PyTorchLightning/metrics) since v1.3 and will be removed in v1.5
  rank_zero_deprecation(
2025-01-10 12:05:27.715278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-01-10 12:05:27.727926: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-01-10 12:05:27.731821: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-01-10 12:05:27.742917: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-01-10 12:05:32.323555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/mnt/share/yinjunhui/cl/FSCIL-Diff/ldm/models/diffusion/ddpm.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  sd = torch.load(path, map_location="cpu")
/mnt/share/yinjunhui/cl/FSCIL-Diff/models/asp.py:493: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  torch.load(self.args['ldm_ckpt'], map_location="cpu")["state_dict"],
This is for the BaseNet initialization.
modelname, pretrained_vit_b16_224_vpt basicmodelname vit_base_patch16_224
Using VPT model
Using Deep Prompt
After BaseNet initialization.
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 872.30 M params.
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 16, 16) = 1024 dimensions.
making attention of type 'vanilla' with 512 in_channels
Restored from models/ldm/text2img-large/model.ckpt with 0 missing and 2 unexpected keys
Unexpected Keys: ['model_ema.decay', 'model_ema.num_updates']
Setting learning rate to 2.00e-02 =  4 (batchsize) * 5.00e-03 (base_lr)
2025-01-10 12:06:23,641 [trainer.py] => All params: 173596160
2025-01-10 12:06:23,642 [trainer.py] => Trainable params: 1998848
2025-01-10 12:06:23,642 [asp.py] => Learning on 0-5
2025-01-10 12:06:23,647 [asp.py] => training set size: 2500, fc construct set size: 2500
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:34<02:18, 34.58s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:11<01:47, 35.73s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:49<01:13, 36.97s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:29<00:37, 37.94s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:08<00:00, 38.59s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:08<00:00, 37.75s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.88s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.81s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.80s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.96s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 40.02s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.95s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.41s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.30s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.25s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.23s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.24s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.26s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.43s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.36s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.29s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:42<00:40, 40.72s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:24<00:00, 41.31s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:24<00:00, 40.94s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:42<02:48, 42.18s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:24<02:05, 41.98s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:05<01:23, 41.93s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:47<00:41, 41.80s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:28<00:00, 41.65s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:28<00:00, 41.77s/it]
2025-01-10 12:23:11,051 [asp.py] => total parameters: 173600001
2025-01-10 12:23:11,051 [asp.py] => trainable parameters: 2002689
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 3840
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:24:22,033 [asp.py] => Task 0, Epoch 1 => Loss 1.622, Train_accy 59.93, Test_curr_accy 98.00, Test_accy 98.00
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:25:31,934 [asp.py] => Task 0, Epoch 2 => Loss 0.788, Train_accy 89.31, Test_curr_accy 95.20, Test_accy 95.20
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:26:43,097 [asp.py] => Task 0, Epoch 3 => Loss 0.625, Train_accy 91.35, Test_curr_accy 97.80, Test_accy 97.80
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:27:53,554 [asp.py] => Task 0, Epoch 4 => Loss 0.464, Train_accy 92.51, Test_curr_accy 98.80, Test_accy 98.80
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:29:03,952 [asp.py] => Task 0, Epoch 5 => Loss 0.394, Train_accy 92.91, Test_curr_accy 98.40, Test_accy 98.40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:30:14,336 [asp.py] => Task 0, Epoch 6 => Loss 0.341, Train_accy 93.42, Test_curr_accy 98.40, Test_accy 98.40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:31:24,808 [asp.py] => Task 0, Epoch 7 => Loss 0.281, Train_accy 94.73, Test_curr_accy 98.60, Test_accy 98.60
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:32:34,436 [asp.py] => Task 0, Epoch 8 => Loss 0.268, Train_accy 94.36, Test_curr_accy 97.20, Test_accy 97.20
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:33:44,414 [asp.py] => Task 0, Epoch 9 => Loss 0.238, Train_accy 95.24, Test_curr_accy 94.40, Test_accy 94.40
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
anchor samples found
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:34:53,234 [asp.py] => Task 0, Epoch 10 => Loss 0.235, Train_accy 95.27, Test_curr_accy 95.20, Test_accy 95.20
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:35:20,438 [trainer.py] => Top1 curve: [94.0]
2025-01-10 12:35:20,439 [trainer.py] => Average Accuracy (Top1): 94.0   (Harmonic Accuracy): None (Old Acc): 94.0 (New Acc): [] 

2025-01-10 12:35:20,439 [trainer.py] => All params: 173600001
2025-01-10 12:35:20,440 [trainer.py] => Trainable params: 2002689
2025-01-10 12:35:20,441 [asp.py] => Learning on 5-10
2025-01-10 12:35:20,450 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][AClass Inversion:   0%|          | 0/10 [28:57<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:42<02:48, 42.16s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:24<02:06, 42.00s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:04<01:22, 41.36s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:47<00:41, 41.90s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:30<00:00, 42.49s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:30<00:00, 42.18s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:43<02:53, 43.34s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:26<02:08, 42.94s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:08<01:25, 42.76s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:50<00:42, 42.62s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:33<00:00, 42.54s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:33<00:00, 42.67s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:42<02:50, 42.57s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:23<02:05, 41.70s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:03<01:21, 40.90s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:43<00:40, 40.51s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:23<00:00, 40.31s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:23<00:00, 40.70s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.94s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.80s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.50s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.23s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:43<02:55, 43.91s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:27<02:10, 43.50s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:10<01:26, 43.31s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:53<00:43, 43.17s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:35<00:00, 42.98s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:35<00:00, 43.16s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:52:48,984 [asp.py] => total parameters: 173603841
2025-01-10 12:52:48,985 [asp.py] => trainable parameters: 2006529
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 7680
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 12:52:55,673 [trainer.py] => Top1 curve: [94.0, 90.5]
2025-01-10 12:52:55,673 [trainer.py] => Average Accuracy (Top1): 92.25   (Harmonic Accuracy): 90.3796685082873 (Old Acc): 93.8 (New Acc): 87.2 

2025-01-10 12:52:55,674 [trainer.py] => All params: 173603841
2025-01-10 12:52:55,675 [trainer.py] => Trainable params: 2006529
2025-01-10 12:52:55,677 [asp.py] => Learning on 10-15
2025-01-10 12:52:55,688 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:42<02:50, 42.60s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:24<02:07, 42.45s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:06<01:23, 41.95s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:46<00:41, 41.41s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:27<00:00, 40.95s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:27<00:00, 41.40s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:44<02:56, 44.00s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:27<02:10, 43.46s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:10<01:26, 43.21s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:52<00:42, 43.00s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:35<00:00, 42.77s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:35<00:00, 43.01s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:42<02:49, 42.37s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:24<02:06, 42.15s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:05<01:23, 41.89s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:46<00:41, 41.30s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:26<00:00, 40.86s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:26<00:00, 41.28s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.41s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<01:59, 39.95s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:19, 39.97s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.95s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.97s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.99s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.15s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.04s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.01s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:39, 39.99s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.95s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.99s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:10:08,159 [asp.py] => total parameters: 173607681
2025-01-10 13:10:08,161 [asp.py] => trainable parameters: 2010369
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 11520
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:10:17,064 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2]
2025-01-10 13:10:17,064 [trainer.py] => Average Accuracy (Top1): 88.23333333333333   (Harmonic Accuracy): 82.18916967509026 (Old Acc): 91.8 (New Acc): 74.4 

2025-01-10 13:10:17,065 [trainer.py] => All params: 173607681
2025-01-10 13:10:17,066 [trainer.py] => Trainable params: 2010369
2025-01-10 13:10:17,067 [asp.py] => Learning on 15-20
2025-01-10 13:10:17,075 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.03s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.14s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.12s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:40, 40.17s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.22s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.18s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:42, 40.54s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.29s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.26s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.28s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.30s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.44s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.36s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.36s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.34s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.32s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.34s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:42, 40.59s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.16s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.02s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:39, 39.95s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 39.93s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.01s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.04s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:27:05,052 [asp.py] => total parameters: 173611521
2025-01-10 13:27:05,054 [asp.py] => trainable parameters: 2014209
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 15360
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:27:16,711 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7]
2025-01-10 13:27:16,711 [trainer.py] => Average Accuracy (Top1): 85.1   (Harmonic Accuracy): 79.61283950617282 (Old Acc): 91.6 (New Acc): 70.39999999999999 

2025-01-10 13:27:16,712 [trainer.py] => All params: 173611521
2025-01-10 13:27:16,712 [trainer.py] => Trainable params: 2014209
2025-01-10 13:27:16,714 [asp.py] => Learning on 20-25
2025-01-10 13:27:16,731 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)



Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.34s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.41s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.35s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.34s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.36s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.49s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.35s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.32s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.32s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.30s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.32s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.48s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.34s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.32s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.33s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.34s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.39s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.33s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.34s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.35s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.35s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.35s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:42, 40.61s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:21<02:01, 40.49s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.42s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.35s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.35s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.39s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:44:09,748 [asp.py] => total parameters: 173615361
2025-01-10 13:44:09,749 [asp.py] => trainable parameters: 2018049
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 19200
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 13:44:23,737 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84]
2025-01-10 13:44:23,737 [trainer.py] => Average Accuracy (Top1): 83.048   (Harmonic Accuracy): 79.63906056860321 (Old Acc): 91.0 (New Acc): 70.8 

2025-01-10 13:44:23,738 [trainer.py] => All params: 173615361
2025-01-10 13:44:23,739 [trainer.py] => Trainable params: 2018049
2025-01-10 13:44:23,740 [asp.py] => Learning on 25-30
2025-01-10 13:44:23,757 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)




Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.36s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.30s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.26s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.29s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.27s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.28s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.48s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.39s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.34s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.33s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.32s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.34s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.45s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.34s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.31s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.27s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.25s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.28s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.50s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.34s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.33s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.37s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.37s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.09s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.93s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.92s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.92s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.94s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:01:14,122 [asp.py] => total parameters: 173619201
2025-01-10 14:01:14,123 [asp.py] => trainable parameters: 2021889
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 23040
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:01:30,612 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4]
2025-01-10 14:01:30,613 [trainer.py] => Average Accuracy (Top1): 81.10666666666667   (Harmonic Accuracy): 77.2124936515998 (Old Acc): 89.8 (New Acc): 67.72 

2025-01-10 14:01:30,613 [trainer.py] => All params: 173619201
2025-01-10 14:01:30,614 [trainer.py] => Trainable params: 2021889
2025-01-10 14:01:30,615 [asp.py] => Learning on 30-35
2025-01-10 14:01:30,637 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)





Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A[AClass Inversion:   0%|          | 0/10 [1:26:10<?, ?it/s]
Class Inversion:   0%|          | 0/10 [1:08:35<?, ?it/s]
Class Inversion:   0%|          | 0/10 [51:14<?, ?it/s]
Class Inversion:   0%|          | 0/10 [34:14<?, ?it/s]
Class Inversion:   0%|          | 0/10 [17:07<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.60s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<02:00, 40.06s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.98s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.93s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.20s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.01s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.94s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.92s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.94s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.04s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.90s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.92s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.91s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.89s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.07s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.97s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.93s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.88s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.86s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:18:13,039 [asp.py] => total parameters: 173623041
2025-01-10 14:18:13,039 [asp.py] => trainable parameters: 2025729
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 26880
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:18:31,979 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71]
2025-01-10 14:18:31,979 [trainer.py] => Average Accuracy (Top1): 79.47857142857143   (Harmonic Accuracy): 76.2748717948718 (Old Acc): 89.6 (New Acc): 66.4 

2025-01-10 14:18:31,980 [trainer.py] => All params: 173623041
2025-01-10 14:18:31,981 [trainer.py] => Trainable params: 2025729
2025-01-10 14:18:31,982 [asp.py] => Learning on 35-40
2025-01-10 14:18:31,995 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.34s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.29s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.24s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.27s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.27s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.27s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.48s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.28s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.30s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.27s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.26s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.28s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.35s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.25s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.21s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:40, 40.22s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.22s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.23s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.35s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.35s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.30s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.31s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.29s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.31s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.50s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.41s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.36s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.37s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.38s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.39s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:35:24,252 [asp.py] => total parameters: 173626881
2025-01-10 14:35:24,252 [asp.py] => trainable parameters: 2029569
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 30720
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:35:45,389 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62]
2025-01-10 14:35:45,389 [trainer.py] => Average Accuracy (Top1): 78.12125   (Harmonic Accuracy): 75.2254606365159 (Old Acc): 87.6 (New Acc): 65.91428571428571 

2025-01-10 14:35:45,390 [trainer.py] => All params: 173626881
2025-01-10 14:35:45,390 [trainer.py] => Trainable params: 2029569
2025-01-10 14:35:45,391 [asp.py] => Learning on 40-45
2025-01-10 14:35:45,416 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.29s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.26s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.29s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.33s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.19s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.24s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.07s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.97s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.96s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.96s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.97s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.16s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.04s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:19, 39.98s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.97s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.98s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.08s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.03s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:19, 39.99s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.98s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.97s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.99s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.08s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.02s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.96s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.96s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.94s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:52:31,040 [asp.py] => total parameters: 173630721
2025-01-10 14:52:31,041 [asp.py] => trainable parameters: 2033409
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 34560
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 14:52:54,993 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18]
2025-01-10 14:52:54,993 [trainer.py] => Average Accuracy (Top1): 77.01666666666668   (Harmonic Accuracy): 75.00339869281046 (Old Acc): 87.2 (New Acc): 65.8 

2025-01-10 14:52:54,994 [trainer.py] => All params: 173630721
2025-01-10 14:52:54,995 [trainer.py] => Trainable params: 2033409
2025-01-10 14:52:54,996 [asp.py] => Learning on 45-50
2025-01-10 14:52:55,024 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.26s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.29s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.33s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.36s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.35s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.34s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:42, 40.56s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.48s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.41s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.39s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.32s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.37s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.48s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.35s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.36s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.38s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.38s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.42s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.37s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.32s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.31s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.29s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.31s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.44s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:01, 40.43s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:01<01:20, 40.44s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.39s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.36s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:21<00:00, 40.39s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:09:48,881 [asp.py] => total parameters: 173634561
2025-01-10 15:09:48,882 [asp.py] => trainable parameters: 2037249
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 38400
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:10:15,276 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72]
2025-01-10 15:10:15,276 [trainer.py] => Average Accuracy (Top1): 75.787   (Harmonic Accuracy): 72.62343842950625 (Old Acc): 87.2 (New Acc): 62.22222222222222 

2025-01-10 15:10:15,277 [trainer.py] => All params: 173634561
2025-01-10 15:10:15,278 [trainer.py] => Trainable params: 2037249
2025-01-10 15:10:15,279 [asp.py] => Learning on 50-55
2025-01-10 15:10:15,289 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)



Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.34s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.31s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.27s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:40, 40.18s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.12s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.18s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.08s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.98s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.94s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.94s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.95s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.88s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.88s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.87s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.06s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.02s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.98s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.98s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.94s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.15s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<01:59, 39.98s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.95s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.95s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.93s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.96s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:27:00,445 [asp.py] => total parameters: 173638401
2025-01-10 15:27:00,446 [asp.py] => trainable parameters: 2041089
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 42240
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:27:28,539 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07]
2025-01-10 15:27:28,539 [trainer.py] => Average Accuracy (Top1): 74.81272727272727   (Harmonic Accuracy): 72.82732020891925 (Old Acc): 86.4 (New Acc): 62.94 

2025-01-10 15:27:28,540 [trainer.py] => All params: 173638401
2025-01-10 15:27:28,541 [trainer.py] => Trainable params: 2041089
2025-01-10 15:27:28,542 [asp.py] => Learning on 55-60
2025-01-10 15:27:28,555 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)




Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A[A





Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.79s/it][A[A[A[A[A[A





Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.82s/it][A[A[A[A[A[A





Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.86s/it][A[A[A[A[A[A





Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it][A[A[A[A[A[A





Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it][A[A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.87s/it]






Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A[A





Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.11s/it][A[A[A[A[A[A





Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.01s/it][A[A[A[A[A[A





Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.08s/it][A[A[A[A[A[A





Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:40, 40.15s/it][A[A[A[A[A[A





Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.23s/it][A[A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.17s/it]






Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A[A





Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.48s/it][A[A[A[A[A[A





Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:20<02:00, 40.26s/it][A[A[A[A[A[A





Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.15s/it][A[A[A[A[A[A





Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:40<00:40, 40.08s/it][A[A[A[A[A[A





Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.03s/it][A[A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:20<00:00, 40.11s/it]






Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A[A





Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.07s/it][A[A[A[A[A[A





Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.95s/it][A[A[A[A[A[A





Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.87s/it][A[A[A[A[A[A





Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.84s/it][A[A[A[A[A[A





Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it][A[A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.86s/it]






Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A[A





Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.90s/it][A[A[A[A[A[A





Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.83s/it][A[A[A[A[A[A





Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.81s/it][A[A[A[A[A[A





Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.82s/it][A[A[A[A[A[A





Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it][A[A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:44:12,596 [asp.py] => total parameters: 173642241
2025-01-10 15:44:12,597 [asp.py] => trainable parameters: 2044929
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 46080
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 15:44:43,865 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27]
2025-01-10 15:44:43,865 [trainer.py] => Average Accuracy (Top1): 73.85083333333334   (Harmonic Accuracy): 71.33959831391024 (Old Acc): 85.4 (New Acc): 61.25454545454545 

2025-01-10 15:44:43,866 [trainer.py] => All params: 173642241
2025-01-10 15:44:43,867 [trainer.py] => Trainable params: 2044929
2025-01-10 15:44:43,868 [asp.py] => Learning on 60-65
2025-01-10 15:44:43,881 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)






Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A[A[AClass Inversion:   0%|          | 0/10 [1:43:13<?, ?it/s]
Class Inversion:   0%|          | 0/10 [1:26:12<?, ?it/s]
Class Inversion:   0%|          | 0/10 [1:08:58<?, ?it/s]
Class Inversion:   0%|          | 0/10 [51:49<?, ?it/s]
Class Inversion:   0%|          | 0/10 [34:28<?, ?it/s]
Class Inversion:   0%|          | 0/10 [17:15<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.64s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.72s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.74s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:38<00:39, 39.77s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.79s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.76s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.90s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.83s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.80s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.81s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.82s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.82s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.00s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.91s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.92s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 40.00s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.89s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.86s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.87s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.86s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.87s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.92s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.89s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.92s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:01:26,091 [asp.py] => total parameters: 173646081
2025-01-10 16:01:26,093 [asp.py] => trainable parameters: 2048769
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 49920
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:01:59,408 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37]
2025-01-10 16:01:59,408 [trainer.py] => Average Accuracy (Top1): 72.89076923076924   (Harmonic Accuracy): 69.64711348588686 (Old Acc): 84.0 (New Acc): 59.48333333333333 

2025-01-10 16:01:59,409 [trainer.py] => All params: 173646081
2025-01-10 16:01:59,410 [trainer.py] => Trainable params: 2048769
2025-01-10 16:01:59,411 [asp.py] => Learning on 65-70
2025-01-10 16:01:59,426 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:37, 39.30s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.59s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:19, 39.70s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:38<00:39, 39.74s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.79s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.72s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.98s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.89s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.84s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.87s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.06s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.97s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.92s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.91s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.92s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.93s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.01s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.04s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.91s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:18:41,727 [asp.py] => total parameters: 173649921
2025-01-10 16:18:41,728 [asp.py] => trainable parameters: 2052609
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 53760
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:19:17,520 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21]
2025-01-10 16:19:17,520 [trainer.py] => Average Accuracy (Top1): 72.05642857142858   (Harmonic Accuracy): 69.57388596585417 (Old Acc): 83.8 (New Acc): 59.47692307692307 

2025-01-10 16:19:17,521 [trainer.py] => All params: 173649921
2025-01-10 16:19:17,522 [trainer.py] => Trainable params: 2052609
2025-01-10 16:19:17,524 [asp.py] => Learning on 70-75
2025-01-10 16:19:17,542 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:37, 39.42s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:18<01:58, 39.45s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:18, 39.43s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:37<00:39, 39.44s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.45s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.44s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.68s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.56s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:19, 39.50s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:38<00:39, 39.48s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.46s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.49s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.61s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.51s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:18, 39.49s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:38<00:39, 39.49s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.47s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.49s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.59s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.50s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:18, 39.50s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:37<00:39, 39.45s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.46s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.48s/it]


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A

Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.59s/it][A[A

Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.53s/it][A[A

Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:19, 39.51s/it][A[A

Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:38<00:39, 39.49s/it][A[A

Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.48s/it][A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.50s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:35:50,347 [asp.py] => total parameters: 173653761
2025-01-10 16:35:50,349 [asp.py] => trainable parameters: 2056449
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 57600
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:36:28,812 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23]
2025-01-10 16:36:28,812 [trainer.py] => Average Accuracy (Top1): 71.20133333333334   (Harmonic Accuracy): 68.069552874379 (Old Acc): 83.4 (New Acc): 57.50000000000001 

2025-01-10 16:36:28,813 [trainer.py] => All params: 173653761
2025-01-10 16:36:28,814 [trainer.py] => Trainable params: 2056449
2025-01-10 16:36:28,815 [asp.py] => Learning on 75-80
2025-01-10 16:36:28,831 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)


Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A


Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:37, 39.46s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:18<01:58, 39.45s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:58<01:18, 39.44s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:37<00:39, 39.44s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.42s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:17<00:00, 39.43s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.59s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:58, 39.58s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:00<01:20, 40.28s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.80s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:23<00:00, 41.00s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:23<00:00, 40.64s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:41<02:44, 41.23s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:22<02:03, 41.10s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:03<01:21, 40.98s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:43<00:40, 40.87s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:24<00:00, 40.79s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:24<00:00, 40.89s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:41, 40.28s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.83s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.71s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:41<00:40, 40.53s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:22<00:00, 40.87s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:22<00:00, 40.52s/it]



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A


Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:41<02:45, 41.39s/it][A[A[A


Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:22<02:03, 41.14s/it][A[A[A


Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:03<01:22, 41.07s/it][A[A[A


Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:44<00:40, 40.98s/it][A[A[A


Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:25<00:00, 40.92s/it][A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:25<00:00, 41.00s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:53:27,357 [asp.py] => total parameters: 173657601
2025-01-10 16:53:27,358 [asp.py] => trainable parameters: 2060289
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 61440
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 16:54:08,770 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23, 59.35]
2025-01-10 16:54:08,771 [trainer.py] => Average Accuracy (Top1): 70.460625   (Harmonic Accuracy): 68.18433598183881 (Old Acc): 83.2 (New Acc): 57.76 

2025-01-10 16:54:08,772 [trainer.py] => All params: 173657601
2025-01-10 16:54:08,773 [trainer.py] => Trainable params: 2060289
2025-01-10 16:54:08,774 [asp.py] => Learning on 80-85
2025-01-10 16:54:08,792 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)



Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A



Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.88s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.91s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.91s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.91s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.99s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.87s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.84s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.85s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.87s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.87s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.01s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.85s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.05s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.87s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.83s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.83s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.80s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A



Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.92s/it][A[A[A[A



Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.82s/it][A[A[A[A



Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.79s/it][A[A[A[A



Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.82s/it][A[A[A[A



Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it][A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:10:51,313 [asp.py] => total parameters: 173661441
2025-01-10 17:10:51,314 [asp.py] => trainable parameters: 2064129
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 65280
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:11:34,475 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23, 59.35, 58.65]
2025-01-10 17:11:34,476 [trainer.py] => Average Accuracy (Top1): 69.76588235294118   (Harmonic Accuracy): 67.6735057983943 (Old Acc): 83.0 (New Acc): 57.12500000000001 

2025-01-10 17:11:34,477 [trainer.py] => All params: 173661441
2025-01-10 17:11:34,477 [trainer.py] => Trainable params: 2064129
2025-01-10 17:11:34,479 [asp.py] => Learning on 85-90
2025-01-10 17:11:34,546 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)




Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A




Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.80s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.75s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.79s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.78s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.80s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.79s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.99s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.90s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.85s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.97s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.89s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.89s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.04s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.93s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.86s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.83s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.77s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]





Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A[A[A[A[A




Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.93s/it][A[A[A[A[A




Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.85s/it][A[A[A[A[A




Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.84s/it][A[A[A[A[A




Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.83s/it][A[A[A[A[A




Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.82s/it][A[A[A[A[ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:28:16,626 [asp.py] => total parameters: 173665281
2025-01-10 17:28:16,627 [asp.py] => trainable parameters: 2067969
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 69120
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:29:02,184 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23, 59.35, 58.65, 58.19]
2025-01-10 17:29:02,185 [trainer.py] => Average Accuracy (Top1): 69.12277777777778   (Harmonic Accuracy): 67.22123594555752 (Old Acc): 82.4 (New Acc): 56.76470588235294 

2025-01-10 17:29:02,186 [trainer.py] => All params: 173665281
2025-01-10 17:29:02,186 [trainer.py] => Trainable params: 2067969
2025-01-10 17:29:02,188 [asp.py] => Learning on 90-95
2025-01-10 17:29:02,212 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)





Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s][A[A[A[A[AClass Inversion:   0%|          | 0/10 [1:44:18<?, ?it/s]
Class Inversion:   0%|          | 0/10 [1:27:02<?, ?it/s]
Class Inversion:   0%|          | 0/10 [1:09:45<?, ?it/s]
Class Inversion:   0%|          | 0/10 [52:33<?, ?it/s]
Class Inversion:   0%|          | 0/10 [34:53<?, ?it/s]
Class Inversion:   0%|          | 0/10 [17:27<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.64s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.71s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.78s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.77s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.81s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:18<00:00, 39.78s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.03s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.94s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.91s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.88s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 40.00s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.90s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.86s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.84s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.82s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.85s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:39, 39.96s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.92s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.88s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.87s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.88s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s]Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it]Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.90s/it]Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.86s/it]Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.83s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.83s/it]Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.85s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:45:44,962 [asp.py] => total parameters: 173669121
2025-01-10 17:45:44,963 [asp.py] => trainable parameters: 2071809
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 72960
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 17:46:33,318 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23, 59.35, 58.65, 58.19, 57.45]
2025-01-10 17:46:33,319 [trainer.py] => Average Accuracy (Top1): 68.50842105263159   (Harmonic Accuracy): 66.72932113625421 (Old Acc): 82.4 (New Acc): 56.06666666666668 

2025-01-10 17:46:33,320 [trainer.py] => All params: 173669121
2025-01-10 17:46:33,320 [trainer.py] => Trainable params: 2071809
2025-01-10 17:46:33,321 [asp.py] => Learning on 95-100
2025-01-10 17:46:33,368 [asp.py] => training set size: 25, fc construct set size: 25
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Class Inversion:   0%|          | 0/10 [00:00<?, ?it/s]
Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:39<02:38, 39.71s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.79s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.80s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.83s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.84s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.82s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.07s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.95s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.91s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.89s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.02s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.94s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.93s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.90s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.91s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.92s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.08s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.96s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.95s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.93s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.93s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.94s/it]

Sampling:   0%|          | 0/5 [00:00<?, ?it/s][A
Sampling:  20%|â–ˆâ–ˆ        | 1/5 [00:40<02:40, 40.09s/it][A
Sampling:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:19<01:59, 39.94s/it][A
Sampling:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [01:59<01:19, 39.90s/it][A
Sampling:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:39<00:39, 39.89s/it][A
Sampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.90s/it][ASampling: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:19<00:00, 39.92s/it]
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 18:03:17,217 [asp.py] => total parameters: 173672961
2025-01-10 18:03:17,218 [asp.py] => trainable parameters: 2075649
backbone.TIP 27648
backbone.Prompt_Encoder.fc_mu.0.weight 393216
backbone.Prompt_Encoder.fc_mu.0.bias 256
backbone.Prompt_Encoder.fc_mu.1.weight 589824
backbone.Prompt_Encoder.fc_mu.1.bias 2304
backbone.Prompt_Encoder.fc_std.0.weight 393216
backbone.Prompt_Encoder.fc_std.0.bias 256
backbone.Prompt_Encoder.fc_std.1.weight 589824
backbone.Prompt_Encoder.fc_std.1.bias 2304
fc.weight 76800
fc.sigma 1
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
2025-01-10 18:04:08,070 [trainer.py] => Top1 curve: [94.0, 90.5, 80.2, 75.7, 74.84, 71.4, 69.71, 68.62, 68.18, 64.72, 65.07, 63.27, 61.37, 61.21, 59.23, 59.35, 58.65, 58.19, 57.45, 56.59]
2025-01-10 18:04:08,071 [trainer.py] => Average Accuracy (Top1): 67.9125   (Harmonic Accuracy): 66.07729187409053 (Old Acc): 82.2 (New Acc): 55.24210526315789 

2025-01-10 18:04:08,071 [trainer.py] => 

Class Inversion:   0%|          | 0/10 [35:06<?, ?it/s]
Class Inversion:   0%|          | 0/10 [17:34<?, ?it/s]
